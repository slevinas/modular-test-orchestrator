# Modular Test Orchestrator

A **modular, CI-friendly test orchestration system**.

The  goal is to show how to:

- Prepare remote runners (VMs / containers)
- Use **Ansible playbooks** to install Docker & Python
- Spin up test stacks via `docker-compose`
- Run **pytest** test suites
- Publish results to **Allure** or other reporting systems

Right now this repo is at **Stage 1: Local + CI smoke tests**  
Next stages will gradually add:

- Ansible orchestration under `ansible/`
- A Python orchestration layer under `orchestrator/`
- Example Dockerized test targets under `docker/`
- CI workflows that drive everything end-to-end

---

## Repo Layout (current)

```text
.
â”œâ”€â”€ README.md
â”œâ”€â”€ ansible/          # Ansible inventories, playbooks, roles (coming next)
â”œâ”€â”€ docker/           # Docker & docker-compose test stack (TBD)
â”œâ”€â”€ docs/             # Design notes, diagrams
â”œâ”€â”€ orchestrator/     # Python layer that will call Ansible / Docker
â”œâ”€â”€ tests/            # pytest suites
â”œâ”€â”€ pytest.ini        # pytest config
â””â”€â”€ .github/workflows # CI workflows
````

---

## Quickstart: run tests locally

```bash
# from repo root
python -m venv .venv
source .venv/bin/activate

pip install --upgrade pip
pip install pytest

pytest
```

You should see the example smoke test pass.

---

## CI: GitHub Actions

This repo ships with a very small CI workflow that:

1. Checks out the code
2. Sets up Python
3. Installs pytest
4. Runs the test suite

See: `.github/workflows/ci-smoke.yml`.

---

## Roadmap (high-level)

* [ ] **Stage 1** â€“ Local pytest + basic CI (this commit)
* [ ] **Stage 2** â€“ Add minimal Docker stack and test against a sample service
* [ ] **Stage 3** â€“ Add Ansible playbooks to provision / run tests remotely
* [ ] **Stage 4** â€“ Add Allure publishing and richer reporting
* [ ] **Stage 5** â€“ Documentation + diagrams for LinkedIn / portfolio




Perfect, going with **D** is exactly what I was hoping youâ€™d pick ðŸ™ƒ
Letâ€™s design the **public repo structure** first, then we can slowly â€œpourâ€ pieces of the XPLG system into it in a safe, generic way.

---

## 1. Proposed Repo Name + One-liner

**Repo name idea:** `modular-test-orchestrator`

**Tagline (for GitHub + LinkedIn later):**

> A modular, Ansible-driven test orchestrator for spinning up remote environments, running Pytest suites in Docker, and publishing Allure reports via CI.

We keep it generic (no XPLG, no Flux, no vendor-specific wording).

---

## 2. Target Folder Structure (High-Level)

Hereâ€™s the **outline** I suggest for the new repo:

```text
modular-test-orchestrator/
â”œâ”€â”€ README.md
â”œâ”€â”€ pyproject.toml               # or setup.cfg / setup.py (your choice)
â”œâ”€â”€ pytest.ini
â”œâ”€â”€ .gitignore
â”œâ”€â”€ .github/
â”‚   â””â”€â”€ workflows/
â”‚       â”œâ”€â”€ ci-orchestrator.yml          # main CI pipeline
â”‚       â”œâ”€â”€ ensure-infra.yml             # optional: infra checks
â”‚       â””â”€â”€ run-pytest-and-publish.yml   # example â€œjobâ€
â”œâ”€â”€ ansible/
â”‚   â”œâ”€â”€ inventories/
â”‚   â”‚   â”œâ”€â”€ dev.ini
â”‚   â”‚   â””â”€â”€ example_hosts.ini
â”‚   â”œâ”€â”€ playbooks/
â”‚   â”‚   â”œâ”€â”€ provision-runner.yml         # create dirs, users, etc.
â”‚   â”‚   â”œâ”€â”€ install-docker.yml
â”‚   â”‚   â”œâ”€â”€ deploy-test-stack.yml        # pull images / docker-compose
â”‚   â”‚   â”œâ”€â”€ run-tests.yml                # run pytest inside container
â”‚   â”‚   â””â”€â”€ fetch-allure-results.yml     # pull artifacts back
â”‚   â””â”€â”€ roles/
â”‚       â”œâ”€â”€ docker/
â”‚       â”‚   â”œâ”€â”€ tasks/
â”‚       â”‚   â”‚   â”œâ”€â”€ install.yml
â”‚       â”‚   â”‚   â””â”€â”€ login_registry.yml   # optional
â”‚       â”œâ”€â”€ python-env/
â”‚       â”‚   â””â”€â”€ tasks/
â”‚       â”‚       â””â”€â”€ setup_venv.yml
â”‚       â””â”€â”€ test-runner/
â”‚           â””â”€â”€ tasks/
â”‚               â”œâ”€â”€ run_pytest.yml
â”‚               â””â”€â”€ collect_results.yml
â”œâ”€â”€ orchestrator/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ config.py               # reads YAML/ENV for target hosts, test suite, etc.
â”‚   â”œâ”€â”€ ansible_runner.py       # Python wrapper around ansible-playbook
â”‚   â”œâ”€â”€ ssh_helpers.py          # safe, generic SSH helpers (optional)
â”‚   â””â”€â”€ allure_push.py          # generic Allure upload, adapted from your old code
â”œâ”€â”€ tests/
â”‚   â”œâ”€â”€ conftest.py
â”‚   â”œâ”€â”€ test_example_smoke.py   # small, generic test suite
â”‚   â””â”€â”€ resources/
â”‚       â””â”€â”€ example_data.json
â”œâ”€â”€ docker/
â”‚   â”œâ”€â”€ docker-compose.test.yml   # test stack: app + db + whatever
â”‚   â””â”€â”€ Dockerfile.runner         # image for running pytest
â””â”€â”€ docs/
    â”œâ”€â”€ Architecture.md           # textual + diagrams (weâ€™ll match Benchmaker style)
    â”œâ”€â”€ CI-Pipeline.md
    â””â”€â”€ Ansible-Playbooks.md
```

This mirrors the **spirit** of your XPLG project:

* `.github/workflows` âŸ¶ from your existing CI orchestrator YAMLs
* `ansible/` âŸ¶ replacement for `.ansible` + infra configs
* `orchestrator/` âŸ¶ clean Python layer replacing your orchestration scripts
* `tests/` âŸ¶ a small, generic example instead of the full proprietary â€œfluxâ€ suite
* `docker/` âŸ¶ Docker/compose helpers, but **generic** (no vendor services)

---

## 3. How Your Old Tree Maps Into This

From your original structure:

* `.ci`, `.github/workflows`, `.ansible`
* `scripts/orchestrate_flux_tests*.py` / `.sh`
* `infra/flux-test-ci/configure_git_runner.sh`
* `suites/flux/src/...` (API clients, Jinja templates, task runners, etc.)
* `common/push_to_allure_service.py`

We **donâ€™t copy** all of that 1:1. Instead:

### âœ… Safe to â€œconceptually portâ€:

* The *idea* of:

  * â€œPrepare remote runnerâ€
  * â€œStart docker composeâ€
  * â€œRun pytestâ€
  * â€œCollect resultsâ€
  * â€œPush Allure resultsâ€
* Your Ansible structure: roles, playbooks, inventories
* The orchestration logic (steps, state machine, error handling), but rewritten generically
* The Allure push helper â†’ becomes `orchestrator/allure_push.py`
* The SSH helpers â†’ becomes `orchestrator/ssh_helpers.py`

### âŒ Should *not* port directly:

* Any XPLG-specific naming (flux, product names, internal URL paths, proprietary APIs)
* Real Jinja templates or XML configs for their product
* Real test cases or business-logic assertions from the company
* Any customer data or log paths

Weâ€™ll create **new**, simple test targets instead:

* e.g. a tiny sample FastAPI or Flask app container
* tests verify basic endpoints, small workloads, etc.
* the focus is on **orchestration mechanics**, not business domain.

---

## 4. README Outline (for Later, Not Now)

Once the structure exists, the README for this repo will likely have sections like:

1. **Overview** â€” â€œWhat is this orchestrator?â€
2. **Architecture** â€” diagram: GitHub Actions â†’ Ansible â†’ Remote Host â†’ Docker â†’ Pytest â†’ Allure
3. **Components** â€” `ansible/`, `orchestrator/`, `tests/`, `docker/`
4. **Running Locally** â€” `docker-compose -f docker/docker-compose.test.yml up`
5. **Running via Ansible** â€” `ansible-playbook ansible/playbooks/run-tests.yml -i ansible/inventories/dev.ini`
6. **CI Example** â€” how the GH Action triggers the orchestrator.

Weâ€™ll mirror the polish from Benchmaker-Lite, but oriented around **CI / Ansible / testing** instead of OTEL.

---

## 5. Concrete Next Micro-Step

To keep things simple and momentum high, hereâ€™s what I suggest you do next:

1. **Create a new empty repo locally** (donâ€™t push yet if you want privacy while we shape it):

   ```bash
   mkdir modular-test-orchestrator
   cd modular-test-orchestrator
   git init
   mkdir -p .github/workflows ansible/{inventories,playbooks,roles} orchestrator tests docker docs
   touch README.md pytest.ini .gitignore
   ```
2. Inside `ansible/playbooks/`, create *empty* files:

   ```bash
   touch ansible/playbooks/{provision-runner.yml,install-docker.yml,deploy-test-stack.yml,run-tests.yml,fetch-allure-results.yml}
   ```
3. Inside `.github/workflows/`, create one empty workflow file:

   ```bash
   touch .github/workflows/ci-orchestrator.yml
   ```

Then you can **paste me the new tree output**:

```bash
tree -C -I ".git" .
```

â€¦and Iâ€™ll start **filling in**:

* a minimal `README.md` first
* a simple initial `ci-orchestrator.yml`
* stubbed Ansible playbooks with comments
* and a tiny example test app/test.

---

If youâ€™d rather not touch the shell right now, I can instead:

* write the **exact contents** for `README.md` + initial files so you can just copy-paste them into the new repo.

Up to you, but the strategic answer to your question is:

> **Best next move: scaffold this second projectâ€™s repo (like above), then weâ€™ll gradually pour in generic versions of your XPLG orchestrator logic.**

If you tell me **â€œLetâ€™s scaffold the files nowâ€**, Iâ€™ll give you copy-paste content for:

* `README.md` (v1)
* `.github/workflows/ci-orchestrator.yml` (skeleton)
* `ansible/playbooks/run-tests.yml` (skeleton)
* `orchestrator/ansible_runner.py` (skeleton)


---

Nice, this is a perfect clean scaffold to start from ðŸ‘Œ

Letâ€™s do a **small but complete â€œStage 1â€**:

> âœ… Repo has a clear README,
> âœ… pytest works locally,
> âœ… a basic CI workflow runs tests on GitHub.

Then in the next round weâ€™ll layer on Ansible orchestration + Docker.

---

## 1. Add a GitHub Actions workflow + orchestrator package

Run these commands in the repo:

```bash
cd ~/Zigi-work/repositories/modular-test-orchestrator

# CI folder
mkdir -p .github/workflows

# orchestrator package
mkdir -p orchestrator
touch orchestrator/__init__.py
```

---

## 2. Replace `README.md` with this (v1)

````md
# Modular Test Orchestrator

A **modular, CI-friendly test orchestration system**.

The long-term goal is to show how you can:

- Prepare remote runners (VMs / containers)
- Use **Ansible playbooks** to install Docker & Python
- Spin up test stacks via `docker-compose`
- Run **pytest** test suites
- Publish results to **Allure** or other reporting systems

Right now this repo is at **Stage 1: Local + CI smoke tests**  
Next stages will gradually add:

- Ansible orchestration under `ansible/`
- A Python orchestration layer under `orchestrator/`
- Example Dockerized test targets under `docker/`
- CI workflows that drive everything end-to-end

---

## Repo Layout (current)

```text
.
â”œâ”€â”€ README.md
â”œâ”€â”€ ansible/          # Ansible inventories, playbooks, roles (coming next)
â”œâ”€â”€ docker/           # Docker & docker-compose test stack (TBD)
â”œâ”€â”€ docs/             # Design notes, diagrams
â”œâ”€â”€ orchestrator/     # Python layer that will call Ansible / Docker
â”œâ”€â”€ tests/            # pytest suites
â”œâ”€â”€ pytest.ini        # pytest config
â””â”€â”€ .github/workflows # CI workflows
````

---

## Quickstart: run tests locally

```bash
# from repo root
python -m venv .venv
source .venv/bin/activate

pip install --upgrade pip
pip install pytest

pytest
```

You should see the example smoke test pass.

---

## CI: GitHub Actions

This repo ships with a very small CI workflow that:

1. Checks out the code
2. Sets up Python
3. Installs pytest
4. Runs the test suite

See: `.github/workflows/ci-smoke.yml`.

As we add Ansible and Docker orchestration, weâ€™ll extend CI to:

* provision test runners
* run Ansible playbooks
* publish test artifacts (e.g., Allure results)

---

## Roadmap (high-level)

* [ ] **Stage 1** â€“ Local pytest + basic CI (this commit)
* [ ] **Stage 2** â€“ Add minimal Docker stack and test against a sample service
* [ ] **Stage 3** â€“ Add Ansible playbooks to provision / run tests remotely
* [ ] **Stage 4** â€“ Add Allure publishing and richer reporting
* [ ] **Stage 5** â€“ Documentation + diagrams for LinkedIn / portfolio

````

---

## 3. Minimal `pytest.ini`

Put this into `pytest.ini` (overwrite if needed):

```ini
[pytest]
testpaths = tests
python_files = test_*.py
python_functions = test_*
addopts = -q
````

---

## 4. Add a tiny smoke test

Create `tests/test_smoke_example.py`:

```python
# tests/test_smoke_example.py

def test_repo_is_alive():
    """
    Simple smoke test so that CI has something to run.
    We'll replace / extend this with real orchestration tests later.
    """
    assert 1 + 1 == 2
```

Run locally:

```bash
pytest
```

You should get 1 passing test.

---

## 5. Add the first CI workflow

Create `.github/workflows/ci-smoke.yml`:

```yaml
name: CI - Smoke Tests

on:
  push:
    branches: [ main ]
  pull_request:
    branches: [ main ]

jobs:
  tests:
    runs-on: ubuntu-latest

    steps:
      - name: Checkout repo
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: "3.11"

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install pytest

      - name: Run pytest
        run: pytest
```

Once you commit & push to GitHub, this will give you a **green checkmark pipeline** for the repo â€” already something you can point to.

---

## 6. (Optional now) Clean up docs filename

You have `docs/Curren.md` â€” if thatâ€™s just a scratch file, you can rename it:

```bash
mv docs/Curren.md docs/Current-Status.md
```

And maybe drop a quick note there like:

```md
# Current Status

- Stage 1 scaffold created
- CI smoke test workflow added
- Next: design Ansible playbooks + orchestration flow
```

---

### What next?

After you:

1. Paste these file contents
2. Run `pytest` locally
3. Push to GitHub and see CI go green

â€¦send me your updated `tree -C -I ".git" .` and/or a screenshot of the repo, and weâ€™ll move to **Stage 2**:

> define the **Ansible + orchestration story** in a way that mirrors what you did at XPLG but stays generic & portfolio-friendly.



##### Stage 2

Nice, this is the fun part ðŸ˜Š
Stage 2 = give your orchestrator **something real to test**: a small Dockerized service you can bring up/down and hit from tests.

Iâ€™ll keep this concrete and copy-pasteable.

---

## 1. Target: a tiny FastAPI â€œdemo system under testâ€

**Goal:**
Have a container that:

* Exposes `http://localhost:8080/health`
* Exposes `http://localhost:8080/info`
* Is trivial but realistic enough for â€œmodular test orchestrationâ€

### 1.1. Add target app code

Create:

`docker/test-target/app/main.py`

```python
# docker/test-target/app/main.py
from fastapi import FastAPI
from pydantic import BaseModel
import socket
import os
import time

app = FastAPI(title="Modular Test Orchestrator Demo Target")

START_TIME = time.time()


class EchoRequest(BaseModel):
    message: str


@app.get("/health")
async def health():
    """Simple liveness check."""
    return {"status": "ok"}


@app.get("/info")
async def info():
    """Basic metadata for sanity checks."""
    return {
        "service": "demo-test-target",
        "host": socket.gethostname(),
        "env": os.getenv("APP_ENV", "local"),
        "uptime_seconds": round(time.time() - START_TIME, 2),
    }


@app.post("/echo")
async def echo(body: EchoRequest):
    """Round-trip payload check."""
    return {"echo": body.message}
```

---

## 2. Dockerfile for the target

Create:

`docker/test-target/Dockerfile`

```dockerfile
# docker/test-target/Dockerfile
FROM python:3.11-slim

WORKDIR /app

ENV PYTHONDONTWRITEBYTECODE=1 \
    PYTHONUNBUFFERED=1

# Install runtime deps
RUN apt-get update && apt-get install -y --no-install-recommends \
    curl \
 && rm -rf /var/lib/apt/lists/*

RUN pip install --no-cache-dir fastapi uvicorn[standard]

COPY app ./app

EXPOSE 8080

CMD ["uvicorn", "app.main:app", "--host", "0.0.0.0", "--port", "8080"]
```

---

## 3. docker-compose file to run it locally

Create:

`docker/docker-compose.test-target.yml`

```yaml
version: "3.9"

services:
  test-target:
    build:
      context: ./test-target
      dockerfile: Dockerfile
    container_name: modular-test-target
    ports:
      - "8080:8080"
    environment:
      APP_ENV: "local"
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8080/health"]
      interval: 5s
      timeout: 3s
      retries: 5
      start_period: 5s
```

> This stays nicely inside `docker/` so later Ansible / CI can just say
> â€œbring up the test target via this compose fileâ€.

---

## 4. Add a simple smoke test

Under `tests/`, add:

`tests/test_target_smoke.py`

```python
# tests/test_target_smoke.py
import time
from typing import Final

import httpx
import pytest

BASE_URL: Final[str] = "http://localhost:8080"


def wait_for_healthy(timeout: float = 30.0, interval: float = 1.0) -> None:
    """Retry /health until it responds or timeout is hit."""
    deadline = time.time() + timeout
    last_exc: Exception | None = None

    while time.time() < deadline:
        try:
            resp = httpx.get(f"{BASE_URL}/health", timeout=2.0)
            if resp.status_code == 200 and resp.json().get("status") == "ok":
                return
        except Exception as exc:  # noqa: BLE001
            last_exc = exc
        time.sleep(interval)

    msg = f"Target service did not become healthy within {timeout}s"
    if last_exc:
        msg += f" (last error: {last_exc})"
    raise RuntimeError(msg)


@pytest.mark.smoke
def test_health_and_info():
    wait_for_healthy()

    health = httpx.get(f"{BASE_URL}/health", timeout=5.0)
    assert health.status_code == 200
    assert health.json().get("status") == "ok"

    info = httpx.get(f"{BASE_URL}/info", timeout=5.0)
    assert info.status_code == 200
    data = info.json()
    assert data["service"] == "demo-test-target"
    assert "uptime_seconds" in data


@pytest.mark.smoke
def test_echo_round_trip():
    wait_for_healthy()

    payload = {"message": "hello-orchestrator"}
    resp = httpx.post(f"{BASE_URL}/echo", json=payload, timeout=5.0)
    assert resp.status_code == 200
    assert resp.json() == {"echo": "hello-orchestrator"}
```

And add `httpx` as a test dependency (either in `requirements-dev.txt` or whatever youâ€™ll use).

---

## 5. How to run this locally

From repo root:

```bash
# 1) Build & start the test target
cd docker
docker compose -f docker-compose.test-target.yml up --build -d

# 2) Run tests from repo root
cd ..
pytest -m smoke -vv

# 3) Tear down when done
cd docker
docker compose -f docker-compose.test-target.yml down
```

If you open a browser to `http://localhost:8080/docs` you should see the small FastAPI app as well.

---

## 6. Why this is a good Stage 2 milestone

With just these pieces you now have:

* A **concrete â€œsystem under testâ€** living under `docker/`
* A **repeatable local workflow**: bring up container â†’ run tests â†’ tear down
* A natural anchor for the *next* steps:

  * CI job that builds this image and runs the smoke tests on GitHub hosted runners
  * Ansible playbooks that can deploy this same container to a remote VM and run the same tests there

If youâ€™d like, next step after you wire this in and test it:

* You send me your updated `tree` and maybe a screenshot of:

  * `docker compose up`
  * `pytest -m smoke` passing

Then we can move to **Stage 3: minimal CI workflow** that uses this target.



Nice ðŸŽ‰ That screenshot is exactly what we wanted:

* `docker compose â€¦ up --build` â†’ test-target container running
* `pytest -m smoke -vv` â†’ both smoke tests **PASSED**
* Only warnings are about the `smoke` mark not being registered.

Letâ€™s clean that up and then wire this into CI in a way that *doesnâ€™t* require a self-hosted runner.

---

## 1ï¸âƒ£ Silence the `smoke` marker warnings

Edit **`pytest.ini`** at the repo root and make sure it looks like this:

```ini
[pytest]
testpaths = tests

markers =
    smoke: basic smoke tests against the test target
```

Now rerun:

```bash
pytest -m smoke -vv
```

The warnings about `Unknown pytest.mark.smoke` should disappear.

---

## 2ï¸âƒ£ Document the test-target in `README.md`

Add a small section so recruiters (and Future You) see how this hangs together.

Somewhere under your main intro, add:

````md
## ðŸ§ª Local Test Target

This repo includes a small **FastAPI test target** container and a set of
pytest smoke tests that verify it end-to-end.

**Start the test target:**

```bash
cd docker
docker compose -f docker-compose.test-target.yml up --build
````

This launches a FastAPI app exposing:

* `GET /health` â€“ simple liveness check
* `GET /info` â€“ basic metadata about the running container
* `POST /echo` â€“ round-trip payload echo

**Run the smoke tests:**

```bash
cd ..
pytest -m smoke -vv
```

The smoke suite uses `httpx.AsyncClient` to call the container and assert that
the API behaves as expected.

````

---

## 3ï¸âƒ£ Add a minimal GitHub Actions workflow (no self-hosted runner needed)

You *can* run this on GitHubâ€™s own runners; no extra setup required.

Create: **`.github/workflows/ci-test-target.yml`**:

```yaml
name: CI - Test Target Smoke

on:
  push:
    branches: [ main ]
  pull_request:
    branches: [ main ]

jobs:
  smoke-tests:
    runs-on: ubuntu-latest

    steps:
      - name: Checkout repo
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: "3.11"

      - name: Install test dependencies
        run: |
          python -m pip install --upgrade pip
          pip install pytest httpx anyio

      - name: Build and run test target
        working-directory: docker
        run: |
          docker compose -f docker-compose.test-target.yml up -d --build
          # give it a few seconds to boot
          sleep 10

      - name: Run smoke tests
        env:
          TEST_TARGET_BASE_URL: http://localhost:8080
        run: |
          pytest -m smoke -vv

      - name: Tear down test target
        if: always()
        working-directory: docker
        run: |
          docker compose -f docker-compose.test-target.yml down
````

This will:

1. Spin up a GitHub-hosted Ubuntu runner.
2. Install Python + pytest/httpx.
3. Build & start your `test-target` container via `docker compose`.
4. Run the `smoke` tests against `http://localhost:8080`.
5. Always tear down the container.

Once you commit + push that, you should see a **â€œCI - Test Target Smokeâ€** check appear on PRs and pushes.

---

If youâ€™d like, next we can:

* Add a prettier overview diagram + README polish for this repo, **or**
* Start on the **Ansible orchestration stage** (where we use playbooks instead of shell scripts to run the same flow).


#### after my remarks...

Nice, youâ€™re moving fast ðŸ˜Š

Youâ€™re exactly right about the CI error: `httpx` simply isnâ€™t installed on the GitHub runner.

Letâ€™s do your three asks one by one.

---

## 1ï¸âƒ£ README â€” Suggested Table of Contents

For **modular-test-orchestrator**, Iâ€™d structure the README like this:

```md
# Modular Test Orchestrator

## Table of Contents

- [Overview](#overview)
- [Architecture](#architecture)
- [Repository Layout](#repository-layout)
- [Local Development](#local-development)
  - [Prerequisites](#prerequisites)
  - [Start the Test Target](#start-the-test-target)
  - [Run Smoke Tests](#run-smoke-tests)
- [CI Pipeline](#ci-pipeline)
- [Ansible Automation (Future Stage)](#ansible-automation-future-stage)
- [Roadmap](#roadmap)
- [License](#license)
```

Then you can fill sections roughly like:

* **Overview** â€“ one-paragraph â€œwhat this repo is and why it existsâ€.
* **Architecture** â€“ short diagram/text: *pytest orchestrator â†’ Docker test target (FastAPI)*; later: *Ansible â†’ remote hosts â†’ pytest*.
* **Repository Layout** â€“ short bullet list of `ansible/`, `docker/`, `orchestrator/`, `tests/`, etc.
* **Local Development** â€“ exact commands for running the compose file and pytest.
* **CI Pipeline** â€“ explain the GH Actions workflow (what it does, when it runs).
* **Ansible Automation (Future Stage)** â€“ a teaser for Stage 3.
* **Roadmap** â€“ checkboxes like â€œAdd Ansible playbooksâ€, â€œAdd Allure integrationâ€, etc.
* **License** â€“ MIT or whatever you choose.

---

## 2ï¸âƒ£ Fixing CI: install dependencies from `requirements.txt`

The error:

```text
E   ModuleNotFoundError: No module named 'httpx'
```

means the CI job didnâ€™t install `httpx` (and probably a bunch of other deps).

#### Using requirements.txt 

Since you now have a `requirements.txt`, letâ€™s make the workflow install from that instead of hard-coding packages.

Update your workflow (e.g. `.github/workflows/ci-test-target.yml`) to something like this:

```yaml
name: CI - Test Target Smoke

on:
  push:
    branches: [ main ]
  pull_request:
    branches: [ main ]

jobs:
  smoke-tests:
    runs-on: ubuntu-latest

    steps:
      - name: Checkout repo
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: "3.11"

      - name: Install dependencies
        working-directory: ${{ github.workspace }}
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt

      - name: Build and run test target
        working-directory: ${{ github.workspace }}/docker
        run: |
          docker compose -f docker-compose.test-target.yml up -d --build
          # give it a few seconds to boot
          sleep 10

      - name: Run smoke tests
        working-directory: ${{ github.workspace }}
        env:
          TEST_TARGET_BASE_URL: http://localhost:8080
        run: |
          pytest -m smoke -vv

      - name: Tear down test target
        if: always()
        working-directory: ${{ github.workspace }}/docker
        run: |
          docker compose -f docker-compose.test-target.yml down
```

Key bits:

* `working-directory: ${{ github.workspace }}` ensures commands run from the repo root.
* `pip install -r requirements.txt` will install `httpx` and all other packages you froze.
* Docker steps run in `â€¦/docker` so the compose file is found.

Once you commit & push this, the next CI run should pass collection and actually run your `smoke` tests.

*(Later we can trim `requirements.txt` down from full `pip freeze` to just the essentials, but itâ€™s fine for now while weâ€™re wiring things up.)*

---

If youâ€™d like, next we can:

* Draft the actual README content using that ToC, **or**
* Add a second CI job (e.g. `pytest -q` for all tests, or `flake8`/`ruff`), so the repo looks even more â€œproduction-yâ€ to reviewers.
